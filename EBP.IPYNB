
EMPLOYEE BURNOUT PREDICTION

IMPORTING NECESSARY LIBRARIES


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
from google.colab import files
uploaded = files.upload()

     
Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Saving employee_burnout_analysis-AI.xlsx to employee_burnout_analysis-AI (4).xlsx
LOADING DATASET


data=pd.read_excel(r"employee_burnout_analysis-AI.xlsx")
     
DATA OVERVIEW


data.head()

     
Employee ID	Date of Joining	Gender	Company Type	WFH Setup Available	Designation	Resource Allocation	Mental Fatigue Score	Burn Rate
0	fffe32003000360033003200	2008-09-30	Female	Service	No	2	3.0	3.8	0.16
1	fffe3700360033003500	2008-11-30	Male	Service	Yes	1	2.0	5.0	0.36
2	fffe31003300320037003900	2008-03-10	Female	Product	Yes	2	NaN	5.8	0.49
3	fffe32003400380032003900	2008-11-03	Male	Service	Yes	1	1.0	2.6	0.20
4	fffe31003900340031003600	2008-07-24	Female	Service	No	3	7.0	6.9	0.52

data.tail()
     
Employee ID	Date of Joining	Gender	Company Type	WFH Setup Available	Designation	Resource Allocation	Mental Fatigue Score	Burn Rate
22745	fffe31003500370039003100	2008-12-30	Female	Service	No	1	3.0	NaN	0.41
22746	fffe33003000350031003800	2008-01-19	Female	Product	Yes	3	6.0	6.7	0.59
22747	fffe390032003000	2008-11-05	Male	Service	Yes	3	7.0	NaN	0.72
22748	fffe33003300320036003900	2008-01-10	Female	Service	No	2	5.0	5.9	0.52
22749	fffe3400350031003800	2008-01-06	Male	Product	No	3	6.0	7.8	0.61

data.describe()
     
Date of Joining	Designation	Resource Allocation	Mental Fatigue Score	Burn Rate
count	22750	22750.000000	21369.000000	20633.000000	21626.000000
mean	2008-07-01 09:28:05.274725120	2.178725	4.481398	5.728188	0.452005
min	2008-01-01 00:00:00	0.000000	1.000000	0.000000	0.000000
25%	2008-04-01 00:00:00	1.000000	3.000000	4.600000	0.310000
50%	2008-07-02 00:00:00	2.000000	4.000000	5.900000	0.450000
75%	2008-09-30 00:00:00	3.000000	6.000000	7.100000	0.590000
max	2008-12-31 00:00:00	5.000000	10.000000	10.000000	1.000000
std	NaN	1.135145	2.047211	1.920839	0.198226

data.columns.tolist()
     
['Employee ID',
 'Date of Joining',
 'Gender',
 'Company Type',
 'WFH Setup Available',
 'Designation',
 'Resource Allocation',
 'Mental Fatigue Score',
 'Burn Rate']

data.nunique()
     
Employee ID             22750
Date of Joining           366
Gender                      2
Company Type                2
WFH Setup Available         2
Designation                 6
Resource Allocation        10
Mental Fatigue Score      101
Burn Rate                 101
dtype: int64

data.info()
     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 22750 entries, 0 to 22749
Data columns (total 9 columns):
 #   Column                Non-Null Count  Dtype         
---  ------                --------------  -----         
 0   Employee ID           22750 non-null  object        
 1   Date of Joining       22750 non-null  datetime64[ns]
 2   Gender                22750 non-null  object        
 3   Company Type          22750 non-null  object        
 4   WFH Setup Available   22750 non-null  object        
 5   Designation           22750 non-null  int64         
 6   Resource Allocation   21369 non-null  float64       
 7   Mental Fatigue Score  20633 non-null  float64       
 8   Burn Rate             21626 non-null  float64       
dtypes: datetime64[ns](1), float64(3), int64(1), object(4)
memory usage: 1.6+ MB

data.isnull().sum()
     
Employee ID                0
Date of Joining            0
Gender                     0
Company Type               0
WFH Setup Available        0
Designation                0
Resource Allocation     1381
Mental Fatigue Score    2117
Burn Rate               1124
dtype: int64

data.isnull().sum().values.sum()
     
4622
EXPLORATORY DATA ANALYSIS


data.corr(numeric_only=True)['Burn Rate'][:-1]
     
Designation             0.737556
Resource Allocation     0.856278
Mental Fatigue Score    0.944546
Name: Burn Rate, dtype: float64

sns.pairplot(data)
plt.show()
     


data=data.dropna()
     

data.shape
     
(18590, 9)

data.dtypes
     
Employee ID                     object
Date of Joining         datetime64[ns]
Gender                          object
Company Type                    object
WFH Setup Available             object
Designation                      int64
Resource Allocation            float64
Mental Fatigue Score           float64
Burn Rate                      float64
dtype: object

data_obj = data.select_dtypes(object)
print({c: data_obj[c].unique()[:10] for c in data_obj.columns})
     
{'Employee ID': array(['fffe32003000360033003200', 'fffe3700360033003500',
       'fffe32003400380032003900', 'fffe31003900340031003600',
       'fffe3300350037003500', 'fffe33003300340039003100',
       'fffe32003600320037003400', 'fffe33003100330032003700',
       'fffe3400310035003800', 'fffe33003100330036003300'], dtype=object), 'Gender': array(['Female', 'Male'], dtype=object), 'Company Type': array(['Service', 'Product'], dtype=object), 'WFH Setup Available': array(['No', 'Yes'], dtype=object)}

print(data.columns)
     
Index(['Employee ID', 'Date of Joining', 'Gender', 'Company Type',
       'WFH Setup Available', 'Designation', 'Resource Allocation',
       'Mental Fatigue Score', 'Burn Rate'],
      dtype='object')

data=data.drop('Employee ID',axis=1)
     
CORRELATION OF DATE OF JOINING WITH TARGET VARIABLE


print(f"Min data {data['Date of Joining'].min()}")
print(f"Max data {data['Date of Joining'].max()}")
data.month=data.copy()

data.month['Date of Joining']=data.month['Date of Joining'].astype("datetime64[ns]")
data.month['Date of Joining'].groupby(data.month['Date of Joining'].dt.month).count().plot(kind="bar",xlabel="Month",ylabel="Hired Employees")
     
Min data 2008-01-01 00:00:00
Max data 2008-12-31 00:00:00
<ipython-input-226-d30677d45884>:3: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access
  data.month=data.copy()
<Axes: xlabel='Month', ylabel='Hired Employees'>


data_2008=pd.to_datetime(["2008-01-01"]*len(data))
data["Days"]=data['Date of Joining'].astype("datetime64[ns]").sub(data_2008).dt.days
data.Days
     
0        273
1        334
3        307
4        205
5        330
        ... 
22743    349
22744    147
22746     18
22748      9
22749      5
Name: Days, Length: 18590, dtype: int64

numeric_data=data.select_dtypes(include=['number'])
correlation=numeric_data.corr()['Burn Rate']
print(correlation)
     
Designation             0.736412
Resource Allocation     0.855005
Mental Fatigue Score    0.944389
Burn Rate               1.000000
Days                    0.000309
Name: Burn Rate, dtype: float64

data.corr(numeric_only=True)['Burn Rate'][:-1]
     
Designation             0.736412
Resource Allocation     0.855005
Mental Fatigue Score    0.944389
Burn Rate               1.000000
Name: Burn Rate, dtype: float64
We observed that there is no strong correlation between Date of Joining and Burn Rate.So, we are dropping the column Date of Joining


data=data.drop(['Date of Joining','Days'],axis=1)
     

data.head()
     
Gender	Company Type	WFH Setup Available	Designation	Resource Allocation	Mental Fatigue Score	Burn Rate
0	Female	Service	No	2	3.0	3.8	0.16
1	Male	Service	Yes	1	2.0	5.0	0.36
3	Male	Service	Yes	1	1.0	2.6	0.20
4	Female	Service	No	3	7.0	6.9	0.52
5	Male	Product	Yes	2	4.0	3.6	0.29
Now analysing the categorical variables


cat_columns=data.select_dtypes(object).columns
fig,ax=plt.subplots(nrows=1,ncols=len(cat_columns),sharey=True,figsize=(10,5))
for i,c in enumerate(cat_columns):
    sns.countplot(x=c,data=data,ax=ax[i])
plt.show()
     

The number of observations of each category on each variable is equally distributed, except to the Company_Type where the number of service jobs its almost twice that of product ones.


for c in data.select_dtypes(object).columns:
    sns.pairplot(data, hue=c)
     




plt.show()
     
One-Hot Encoding for categorical features


if all(col in data.columns for col in ['Company Type','WFH Setup Available','Gender']):
  data=pd.get_dummies(data,columns=['Company Type','WFH Setup Available','Gender'],drop_first=True)
  data.head()
  encoded_columns=data.columns
else:
  print("Error: One or more of the specified columns are not present in the DataFrame.")
  encoded_columns=None
  print(data.columns)
     

# Calculating the IQR only for numerical columns
numerical_columns = data.select_dtypes(include=[np.number]).columns

Q1 = data[numerical_columns].quantile(0.25)
Q3 = data[numerical_columns].quantile(0.75)
IQR = Q3 - Q1

# Filtering out the outliers for numerical columns
data = data[~((data[numerical_columns] < (Q1 - 1.5 * IQR)) | (data[numerical_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]

print(data.shape)
     
(18215, 7)

data.corr(numeric_only=True)['Burn Rate'][:-1]
     
Designation                0.719458
Resource Allocation        0.845620
Mental Fatigue Score       0.941141
Burn Rate                  1.000000
Company Type_Service       0.003784
WFH Setup Available_Yes   -0.314860
Name: Burn Rate, dtype: float64

data=data.drop('Company Type_Service', axis=1)
data=data.drop('WFH Setup Available_Yes', axis=1)
data=data.drop('Gender_Male', axis=1)
     

# @title Preprocessing

#split df into X and Y
Y=data['Burn Rate']
X=data.drop('Burn Rate',axis=1)
     

#Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, shuffle=True, random_state=1)

# scale X
scaler= MinMaxScaler()
scaler.fit(X_train)
X_train=pd.DataFrame(scaler.transform(X_train),index=X_train.index,columns=X_train.columns)
X_test=pd.DataFrame(scaler.transform(X_test),index=X_test.index,columns=X_test.columns)
     

import os
import pickle

scaler_filename='../models/scaler.pkl'

os.makedirs(os.path.dirname(scaler_filename), exist_ok=True)
with open(scaler_filename, 'wb') as file:
    pickle.dump(scaler, file)
     

X_train
     
Designation	Resource Allocation	Mental Fatigue Score
8577	0.8	0.666667	0.714286
18389	0.2	0.333333	0.549451
4038	0.2	0.222222	0.186813
12406	0.6	0.555556	0.912088
5146	0.4	0.222222	0.175824
...	...	...	...
13735	0.4	0.555556	0.604396
21623	0.6	0.444444	0.604396
6471	0.6	0.666667	0.857143
15260	0.2	0.222222	0.615385
294	0.4	0.444444	0.483516
12750 rows × 3 columns


Y_train
     
8577     0.76
18389    0.41
4038     0.20
12406    0.84
5146     0.11
         ... 
13735    0.57
21623    0.42
6471     0.70
15260    0.46
294      0.39
Name: Burn Rate, Length: 12750, dtype: float64

import os
import pickle

#saving the processed data
path='../data/processed/'

#create the directory if it doesn't exist
os.makedirs(path, exist_ok=True)

X_train.to_csv(path+'X_train_processed.csv', index=False)
Y_train.to_csv(path+'Y_train_processed.csv', index=False)
     
MODEL BUILDING


# @title Linear Regression

#from sklearn.linear_model import LinearRegression

#create an instance of the LinearRegression class
linear_regression_model = LinearRegression()

#Train the data
linear_regression_model.fit(X_train,Y_train)
     
LinearRegression()
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

#Linear Regressing Model Performance Metrics

print("Linear Regression Model Performance Metrics:\n")
#make predictions on the test set
Y_pred=linear_regression_model.predict(X_test)

#calculate mean squared error
mse=mean_squared_error(Y_test,Y_pred)
print("Mean Squared Error:",mse)

#calculate root mean squared error
rmse=mean_squared_error(Y_test,Y_pred,squared=False)
print("Root Mean Squared Error:",rmse)

#calculate mean absolute error
mae=mean_absolute_error(Y_test,Y_pred)
print("Mean Absolute Error:",mae)

#calculate R-squared score
r2=r2_score(Y_test,Y_pred)
print("R-Squared score:",r2)

     
Linear Regression Model Performance Metrics:

Mean Squared Error: 0.0030280995140975356
Root Mean Squared Error: 0.05502817018671015
Mean Absolute Error: 0.04509190829035063
R-Squared score: 0.9128960217701027
Based in the evaluation metrics, the Linear Regression model appears to be the best model for predicting burnout analysis.

It has the lowest mean squared error, root mean squared error and mean absolute error, indicating better accuracy and precision in its predictions. Additionally, it has the highest R-squared score, indicating a good fit to the data and explaining a higher proportion of the variance in the target variable.

So we are choosing this model for deployment.


data
     
Designation	Resource Allocation	Mental Fatigue Score	Burn Rate
0	2	3.0	3.8	0.16
1	1	2.0	5.0	0.36
3	1	1.0	2.6	0.20
4	3	7.0	6.9	0.52
5	2	4.0	3.6	0.29
...	...	...	...	...
22743	1	3.0	6.0	0.48
22744	3	7.0	6.2	0.54
22746	3	6.0	6.7	0.59
22748	2	5.0	5.9	0.52
22749	3	6.0	7.8	0.61
18215 rows × 4 columns


new_data = {
    'Designation': [0.5751],
    'Resource Allocation': [0.6247],
    'Mental Fatigue Score': [0.7813]
}
new_inp = pd.DataFrame(new_data)
new_predict = linear_regression_model.predict(new_inp)
print("Prediction of burnout of an employee is:", new_predict)
     
Prediction of burnout of an employee is: [0.68183479]
